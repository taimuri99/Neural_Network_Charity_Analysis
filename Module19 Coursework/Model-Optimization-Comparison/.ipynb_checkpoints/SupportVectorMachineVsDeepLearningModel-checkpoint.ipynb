{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815ff9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVMs are a type of binary classifier that use geometric boundaries to distinguish data points from two separate groups. More specifically, SVMs try to calculate a geometric hyperplane that maximizes the distance between the closest data point of both groups:\n",
    "\n",
    "# The plot shows an optimal hyperplane, calculated by\n",
    "# SVMs.\n",
    "\n",
    "# Unlike logistic regression, which excels in classifying data that is linearly separable but fails in nonlinear relationships, SVMs can build adequate models with linear or nonlinear data. Due to SVMs' ability to create multidimensional borders, SVMs lose their interpretability and behave more like the black box machine learning models, such as basic neural networks and deep learning models.\n",
    "\n",
    "# SVMs, like neural networks, can analyze and interpret multiple data types, such as images, natural language voice and text, or tabular data. SVMs perform one task and one task very well—they classify and create regression using two groups. In contrast, neural networks and deep learning models are capable of producing many outputs, which means neural network models can be used to classify multiple groups within the same model. Over the years, techniques have been developed to create multiple SVM models side-by-side for multiple classification problems, such as creating multiple SVM kernels. However, a single SVM is not capable of the same outputs as a single neural network.\n",
    "\n",
    "# If we only compare binary classification problems, SVMs have an advantage over neural network and deep learning models:\n",
    "\n",
    "# Neural networks and deep learning models will often converge on a local minimum. In other words, these models will often focus on a specific trend in the data and could miss the \"bigger picture.\"\n",
    "# SVMs are less prone to overfitting because they are trying to maximize the distance, rather than encompass all data within a boundary.\n",
    "# Despite these advantages, SVMs are limited in their potential and can still miss critical features and high-dimensionality relationships that a well-trained deep learning model could find. However, in many straightforward binary classification problems, SVMs will outperform the basic neural network, and even deep learning models with ease.\n",
    "\n",
    "# To compare and contrast the performance of an SVM versus deep learning model, we'll try to build a binary classifier using the same input data. This adapted real-world datasetLinks to an external site. contains bank telemarketing metrics that can be used to predict whether or not a customer is likely to subscribe to a banking service after being targeted by telemarketing advertisements. From this dataset, we want to build a binary classifier using an SVM and deep learning model and compare the predictive accuracy of either model.\n",
    "\n",
    "# First, we'll download the bank telemarketing dataset (bank_telemarketing.csv)Links to an external site. Notebook. Next, we'll make a new Jupyter Notebook and name it \"SVM_DeepLearning\" (or something similar)—this will help us easily locate the comparison example at another time. Once we have created our notebook and placed the dataset into the corresponding folder, we'll start by importing our libraries and reading in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ede35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Job</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Education</th>\n",
       "      <th>Default_Credit</th>\n",
       "      <th>Housing_Loan</th>\n",
       "      <th>Personal_Loan</th>\n",
       "      <th>Subscribed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>other</td>\n",
       "      <td>married</td>\n",
       "      <td>Primary_Education</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>Secondary_Education</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>admin</td>\n",
       "      <td>married</td>\n",
       "      <td>Primary_Education</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>Secondary_Education</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>admin</td>\n",
       "      <td>married</td>\n",
       "      <td>Professional_Education</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age       Job Marital_Status               Education Default_Credit  \\\n",
       "0   56     other        married       Primary_Education             no   \n",
       "1   37  services        married     Secondary_Education             no   \n",
       "2   40     admin        married       Primary_Education             no   \n",
       "3   56  services        married     Secondary_Education             no   \n",
       "4   59     admin        married  Professional_Education             no   \n",
       "\n",
       "  Housing_Loan Personal_Loan Subscribed  \n",
       "0           no            no         no  \n",
       "1          yes            no         no  \n",
       "2           no            no         no  \n",
       "3           no           yes         no  \n",
       "4           no            no         no  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import our input dataset\n",
    "tele_df = pd.read_csv('Resources/bank_telemarketing.csv')\n",
    "tele_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82dc5d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Job               9\n",
       "Marital_Status    3\n",
       "Education         4\n",
       "Default_Credit    2\n",
       "Housing_Loan      2\n",
       "Personal_Loan     2\n",
       "Subscribed        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unlike neural networks and deep learning models, SVMs can handle unprocessed and processed tabular data. Regardless, we'll preprocesses the dataset and use the preprocessed data for training both models—this ensures a fair comparison. For our first preprocessing workflow, let's encode our categorical variables using Scikit-Learn's OneHotEncoder class.\n",
    "\n",
    "# First, we must make sure that none of our categorical variables require bucketing. To check this, let's get the column names of categorical variables and check their number of unique values. Add and run the following code to the notebook:\n",
    "\n",
    "# Generate our categorical variable list\n",
    "tele_cat = tele_df.dtypes[tele_df.dtypes == \"object\"].index.tolist()\n",
    "\n",
    "\n",
    "# Check the number of unique values in each column\n",
    "tele_df[tele_cat].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dbec40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taimurahmadkhan/opt/anaconda3/envs/mlenv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_admin</th>\n",
       "      <th>Job_blue-collar</th>\n",
       "      <th>Job_entrepreneur</th>\n",
       "      <th>Job_management</th>\n",
       "      <th>Job_other</th>\n",
       "      <th>Job_retired</th>\n",
       "      <th>Job_self-employed</th>\n",
       "      <th>Job_services</th>\n",
       "      <th>Job_technician</th>\n",
       "      <th>Marital_Status_divorced</th>\n",
       "      <th>...</th>\n",
       "      <th>Education_Secondary_Education</th>\n",
       "      <th>Education_Tertiary_Education</th>\n",
       "      <th>Default_Credit_no</th>\n",
       "      <th>Default_Credit_yes</th>\n",
       "      <th>Housing_Loan_no</th>\n",
       "      <th>Housing_Loan_yes</th>\n",
       "      <th>Personal_Loan_no</th>\n",
       "      <th>Personal_Loan_yes</th>\n",
       "      <th>Subscribed_no</th>\n",
       "      <th>Subscribed_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Job_admin  Job_blue-collar  Job_entrepreneur  Job_management  Job_other  \\\n",
       "0        0.0              0.0               0.0             0.0        1.0   \n",
       "1        0.0              0.0               0.0             0.0        0.0   \n",
       "2        1.0              0.0               0.0             0.0        0.0   \n",
       "3        0.0              0.0               0.0             0.0        0.0   \n",
       "4        1.0              0.0               0.0             0.0        0.0   \n",
       "\n",
       "   Job_retired  Job_self-employed  Job_services  Job_technician  \\\n",
       "0          0.0                0.0           0.0             0.0   \n",
       "1          0.0                0.0           1.0             0.0   \n",
       "2          0.0                0.0           0.0             0.0   \n",
       "3          0.0                0.0           1.0             0.0   \n",
       "4          0.0                0.0           0.0             0.0   \n",
       "\n",
       "   Marital_Status_divorced  ...  Education_Secondary_Education  \\\n",
       "0                      0.0  ...                            0.0   \n",
       "1                      0.0  ...                            1.0   \n",
       "2                      0.0  ...                            0.0   \n",
       "3                      0.0  ...                            1.0   \n",
       "4                      0.0  ...                            0.0   \n",
       "\n",
       "   Education_Tertiary_Education  Default_Credit_no  Default_Credit_yes  \\\n",
       "0                           0.0                1.0                 0.0   \n",
       "1                           0.0                1.0                 0.0   \n",
       "2                           0.0                1.0                 0.0   \n",
       "3                           0.0                1.0                 0.0   \n",
       "4                           0.0                1.0                 0.0   \n",
       "\n",
       "   Housing_Loan_no  Housing_Loan_yes  Personal_Loan_no  Personal_Loan_yes  \\\n",
       "0              1.0               0.0               1.0                0.0   \n",
       "1              0.0               1.0               1.0                0.0   \n",
       "2              1.0               0.0               1.0                0.0   \n",
       "3              1.0               0.0               0.0                1.0   \n",
       "4              1.0               0.0               1.0                0.0   \n",
       "\n",
       "   Subscribed_no  Subscribed_yes  \n",
       "0            1.0             0.0  \n",
       "1            1.0             0.0  \n",
       "2            1.0             0.0  \n",
       "3            1.0             0.0  \n",
       "4            1.0             0.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the number of unique values for each categorical variable, there were no categories that require bucketing prior to encoding. Therefore, we're ready to encode by adding and running the following code to our notebooks:\n",
    "\n",
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(tele_df[tele_cat]))\n",
    "\n",
    "# Add the encoded variable names to the dataframe\n",
    "encode_df.columns = enc.get_feature_names(tele_cat)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "603b0cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taimurahmadkhan/opt/anaconda3/envs/mlenv/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Job_admin</th>\n",
       "      <th>Job_blue-collar</th>\n",
       "      <th>Job_entrepreneur</th>\n",
       "      <th>Job_management</th>\n",
       "      <th>Job_other</th>\n",
       "      <th>Job_retired</th>\n",
       "      <th>Job_self-employed</th>\n",
       "      <th>Job_services</th>\n",
       "      <th>Job_technician</th>\n",
       "      <th>...</th>\n",
       "      <th>Education_Secondary_Education</th>\n",
       "      <th>Education_Tertiary_Education</th>\n",
       "      <th>Default_Credit_no</th>\n",
       "      <th>Default_Credit_yes</th>\n",
       "      <th>Housing_Loan_no</th>\n",
       "      <th>Housing_Loan_yes</th>\n",
       "      <th>Personal_Loan_no</th>\n",
       "      <th>Personal_Loan_yes</th>\n",
       "      <th>Subscribed_no</th>\n",
       "      <th>Subscribed_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Job_admin  Job_blue-collar  Job_entrepreneur  Job_management  \\\n",
       "0   56        0.0              0.0               0.0             0.0   \n",
       "1   37        0.0              0.0               0.0             0.0   \n",
       "2   40        1.0              0.0               0.0             0.0   \n",
       "3   56        0.0              0.0               0.0             0.0   \n",
       "4   59        1.0              0.0               0.0             0.0   \n",
       "\n",
       "   Job_other  Job_retired  Job_self-employed  Job_services  Job_technician  \\\n",
       "0        1.0          0.0                0.0           0.0             0.0   \n",
       "1        0.0          0.0                0.0           1.0             0.0   \n",
       "2        0.0          0.0                0.0           0.0             0.0   \n",
       "3        0.0          0.0                0.0           1.0             0.0   \n",
       "4        0.0          0.0                0.0           0.0             0.0   \n",
       "\n",
       "   ...  Education_Secondary_Education  Education_Tertiary_Education  \\\n",
       "0  ...                            0.0                           0.0   \n",
       "1  ...                            1.0                           0.0   \n",
       "2  ...                            0.0                           0.0   \n",
       "3  ...                            1.0                           0.0   \n",
       "4  ...                            0.0                           0.0   \n",
       "\n",
       "   Default_Credit_no  Default_Credit_yes  Housing_Loan_no  Housing_Loan_yes  \\\n",
       "0                1.0                 0.0              1.0               0.0   \n",
       "1                1.0                 0.0              0.0               1.0   \n",
       "2                1.0                 0.0              1.0               0.0   \n",
       "3                1.0                 0.0              1.0               0.0   \n",
       "4                1.0                 0.0              1.0               0.0   \n",
       "\n",
       "   Personal_Loan_no  Personal_Loan_yes  Subscribed_no  Subscribed_yes  \n",
       "0               1.0                0.0            1.0             0.0  \n",
       "1               1.0                0.0            1.0             0.0  \n",
       "2               1.0                0.0            1.0             0.0  \n",
       "3               0.0                1.0            1.0             0.0  \n",
       "4               1.0                0.0            1.0             0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once we have our encoded categorical variables, we need to merge our encoded columns back into our original DataFrame (as well as remove the unencoded columns). To replace the unencoded categorical variables with the encoded variables, add and run the following code to the notebook:\n",
    "\n",
    "# Merge one-hot encoded features and drop the originals\n",
    "tele_df = tele_df.merge(encode_df,left_index=True, right_index=True)\n",
    "tele_df = tele_df.drop(tele_cat,1)\n",
    "tele_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "523a63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we must split our data into the training and testing sets prior to standardization to not incorporate the testing values into the scale—testing values are only for evaluation. To perform the training/test split and standardize our numerical variables, add and run the following code in the notebook:\n",
    "\n",
    "# Remove loan status target from features data\n",
    "y = tele_df.Subscribed_yes.values\n",
    "X = tele_df.drop(columns=[\"Subscribed_no\",\"Subscribed_yes\"]).values\n",
    "\n",
    "# Split training/test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0434e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# After standardizing variables in both the training and testing data, our dataset is ready for both models. First, we'll train and evaluate our SVM.\n",
    "\n",
    "# REWIND\n",
    "# SVMs can be built using Scikit-learn's SVCclass in the svmmodule.\n",
    "\n",
    "# For our purposes, we'll use the SVM's linear kernel to try and create a linear boundary between the \"Subscribed_yes\" versus \"Subscribed_no\" groups. To create our SVM model and test the performance, add and run the following code:\n",
    "\n",
    "# Create the SVM model\n",
    "svm = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "311230d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SVM model accuracy: 0.873\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "svm.fit(X_train, y_train)\n",
    "# Evaluate the model\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "print(f\" SVM model accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bf308d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "# Looking at the output of our SVM model, the model was able to correctly predict the customers who subscribed roughly 87% of the time, which is a respectable first-pass model. Next, we need to compile and evaluate our deep learning model. Again, we'll use our typical binary classifier parameters:\n",
    "\n",
    "# Our first hidden layer will have an input_dim equal to the length of the scaled feature data X , 10 neuron units, and will use the relu activation function.\n",
    "# Our second hidden layer will have 5 neuron units and also will use the relu activation function.\n",
    "# The loss function should be binary_crossentropy, using the adam optimizer.\n",
    "# NOTE\n",
    "# Unlike our basic neural network model, we don't want to use two to three times the number of neurons as input variables—we don't want our deeper layers to overfit the input data.\n",
    "\n",
    "# To build and compile our deep learning model, we must add and run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcca4b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 04:43:09.244047: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-28 04:43:09.244383: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 10\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "302b54b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22857 samples\n",
      "Epoch 1/50\n",
      "22857/22857 [==============================] - 1s 48us/sample - loss: 0.5396 - accuracy: 0.7469\n",
      "Epoch 2/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3768 - accuracy: 0.8733\n",
      "Epoch 3/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3730 - accuracy: 0.8734\n",
      "Epoch 4/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3708 - accuracy: 0.8733\n",
      "Epoch 5/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3694 - accuracy: 0.8735\n",
      "Epoch 6/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3684 - accuracy: 0.8734\n",
      "Epoch 7/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3676 - accuracy: 0.8735\n",
      "Epoch 8/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3666 - accuracy: 0.8735\n",
      "Epoch 9/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3660 - accuracy: 0.8735\n",
      "Epoch 10/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3654 - accuracy: 0.8734\n",
      "Epoch 11/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3650 - accuracy: 0.8736\n",
      "Epoch 12/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3646 - accuracy: 0.8737\n",
      "Epoch 13/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3641 - accuracy: 0.8734\n",
      "Epoch 14/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3640 - accuracy: 0.8734\n",
      "Epoch 15/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3637 - accuracy: 0.8737\n",
      "Epoch 16/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3634 - accuracy: 0.8734\n",
      "Epoch 17/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3632 - accuracy: 0.8736\n",
      "Epoch 18/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3630 - accuracy: 0.8740\n",
      "Epoch 19/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3630 - accuracy: 0.8737\n",
      "Epoch 20/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3627 - accuracy: 0.8737\n",
      "Epoch 21/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3626 - accuracy: 0.8739\n",
      "Epoch 22/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3626 - accuracy: 0.8738\n",
      "Epoch 23/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3623 - accuracy: 0.8740\n",
      "Epoch 24/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3622 - accuracy: 0.8739\n",
      "Epoch 25/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3622 - accuracy: 0.8737\n",
      "Epoch 26/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3622 - accuracy: 0.8736\n",
      "Epoch 27/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3619 - accuracy: 0.8740\n",
      "Epoch 28/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3618 - accuracy: 0.8735\n",
      "Epoch 29/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3619 - accuracy: 0.8735\n",
      "Epoch 30/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3618 - accuracy: 0.8738\n",
      "Epoch 31/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3615 - accuracy: 0.8740\n",
      "Epoch 32/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3616 - accuracy: 0.8740\n",
      "Epoch 33/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3616 - accuracy: 0.8739\n",
      "Epoch 34/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3615 - accuracy: 0.8735\n",
      "Epoch 35/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3615 - accuracy: 0.8734\n",
      "Epoch 36/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3614 - accuracy: 0.8740\n",
      "Epoch 37/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3613 - accuracy: 0.8736\n",
      "Epoch 38/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3615 - accuracy: 0.8736\n",
      "Epoch 39/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3611 - accuracy: 0.8737\n",
      "Epoch 40/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3611 - accuracy: 0.8738\n",
      "Epoch 41/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3612 - accuracy: 0.8733\n",
      "Epoch 42/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3608 - accuracy: 0.8738\n",
      "Epoch 43/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3608 - accuracy: 0.8740\n",
      "Epoch 44/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3610 - accuracy: 0.8736\n",
      "Epoch 45/50\n",
      "22857/22857 [==============================] - 1s 28us/sample - loss: 0.3609 - accuracy: 0.8741\n",
      "Epoch 46/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3604 - accuracy: 0.8738\n",
      "Epoch 47/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3607 - accuracy: 0.8738\n",
      "Epoch 48/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3607 - accuracy: 0.8737\n",
      "Epoch 49/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3605 - accuracy: 0.8740\n",
      "Epoch 50/50\n",
      "22857/22857 [==============================] - 1s 27us/sample - loss: 0.3607 - accuracy: 0.8739\n",
      "7620/1 - 0s - loss: 0.5508 - accuracy: 0.8734\n",
      "Loss: 0.36982308479118847, Accuracy: 0.8733595609664917\n"
     ]
    }
   ],
   "source": [
    "# Lastly, we need to train and evaluate our deep learning model. Because this dataset contains fewer features than other datasets we have used previously, we only need to train over a maximum of 50 epochs. Again, we must add and run the following code:\n",
    "\n",
    "# Train the model \n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=50) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e71a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the results of our comparative analysis, the SVM and deep learning models both achieved a predictive accuracy around 87%. Additionally, both models take similar amounts of time to train on the input data. The only noticeable difference between the two models is implementation—the amount of code required to build and train the SVM is notably less than the comparable deep learning model. # As a result, many data scientists will prefer to use SVMs by default, then turn to deep learning models, as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc13fca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
