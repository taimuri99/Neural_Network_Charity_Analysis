{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bfe1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest classifiers are a type of ensemble learning model that combines multiple smaller models into a more robust and accurate model. Random forest models use a number of weak learner algorithms (decision trees) and combine their output to make a final classification (or regression) decision. Structurally speaking, random forest models are very similar to their neural network counterparts. Random forest models have been a staple in machine learning algorithms for many years due to their robustness and scalability. Both output and feature selection of random forest models are easy to interpret, and they can easily handle outliers and nonlinear data.\n",
    "\n",
    "# An example of a random forest with three decision trees making a\n",
    "# prediction based on a series of true and false\n",
    "# questions.\n",
    "\n",
    "\n",
    "\n",
    "# SKILL DRILL\n",
    "# Take a moment to consider a few different reasons to the following question:\n",
    "\n",
    "# If random forest models are fairly robust and clear, why would you want to replace them with a neural network?\n",
    "\n",
    "# End of text box.\n",
    "# The answer depends on the type and complexity of the entire dataset. First and foremost, random forest models will only handle tabular data, so data such as images or natural language data cannot be used in a random forest without heavy modifications to the data. Neural networks can handle all sorts of data types and structures in raw format or with general transformations (such as converting categorical data).\n",
    "\n",
    "# In addition, each model handles input data differently. Random forest models are dependent on each weak learner being trained on a subset of the input data. Once each weak learner is trained, the random forest model predicts the classification based on a consensus of the weak learners. In contrast, deep learning models evaluate input data within a single neuron, as well as across multiple neurons and layers.\n",
    "\n",
    "# As a result, the deep learning model might be able to identify variability in a dataset that a random forest model could miss. However, a random forest model with a sufficient number of estimators and tree depth should be able to perform at a similar capacity to most deep learning models.\n",
    "\n",
    "# To compare the implementation and performance of a random forest model versus a deep learning model, we'll train and evaluate both models on the same data. This time, we'll use a dataset that has been adapted from bank loan dataLinks to an external site. with more than 36,000 rows and 16 feature columns. From this dataset, we want to build a classifier that can predict whether or not a loan will be paid provided their current loan status and metrics.\n",
    "\n",
    "# First, we'll download the bank loan status dataset (loan_status.csv)Links to an external site. Notebook. Next, we'll make a new Jupyter Notebook and name it \"RandomForest_DeepLearning\" (or something similar). This will help us easily locate the comparison example at another time. Once we have created our notebook and placed the dataset into the corresponding folder, we'll start by importing our libraries and reading in the dataset. Copy and run the following code into the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aa19fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_Status</th>\n",
       "      <th>Current_Loan_Amount</th>\n",
       "      <th>Term</th>\n",
       "      <th>Credit_Score</th>\n",
       "      <th>Annual_Income</th>\n",
       "      <th>Years_in_current_job</th>\n",
       "      <th>Home_Ownership</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Monthly_Debt</th>\n",
       "      <th>Years_of_Credit_History</th>\n",
       "      <th>Months_since_last_delinquent</th>\n",
       "      <th>Number_of_Open_Accounts</th>\n",
       "      <th>Number_of_Credit_Problems</th>\n",
       "      <th>Current_Credit_Balance</th>\n",
       "      <th>Maximum_Open_Credit</th>\n",
       "      <th>Bankruptcies</th>\n",
       "      <th>Tax_Liens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fully_Paid</td>\n",
       "      <td>99999999</td>\n",
       "      <td>Short_Term</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2231892.0</td>\n",
       "      <td>8_years</td>\n",
       "      <td>Own_Home</td>\n",
       "      <td>Debt_Consolidation</td>\n",
       "      <td>29200.53</td>\n",
       "      <td>14.9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>297996</td>\n",
       "      <td>750090.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fully_Paid</td>\n",
       "      <td>217646</td>\n",
       "      <td>Short_Term</td>\n",
       "      <td>730.0</td>\n",
       "      <td>1184194.0</td>\n",
       "      <td>&lt;_1_year</td>\n",
       "      <td>Home_Mortgage</td>\n",
       "      <td>Debt_Consolidation</td>\n",
       "      <td>10855.08</td>\n",
       "      <td>19.6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>122170</td>\n",
       "      <td>272052.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fully_Paid</td>\n",
       "      <td>548746</td>\n",
       "      <td>Short_Term</td>\n",
       "      <td>678.0</td>\n",
       "      <td>2559110.0</td>\n",
       "      <td>2_years</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Debt_Consolidation</td>\n",
       "      <td>18660.28</td>\n",
       "      <td>22.6</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>437171</td>\n",
       "      <td>555038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fully_Paid</td>\n",
       "      <td>99999999</td>\n",
       "      <td>Short_Term</td>\n",
       "      <td>728.0</td>\n",
       "      <td>714628.0</td>\n",
       "      <td>3_years</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Debt_Consolidation</td>\n",
       "      <td>11851.06</td>\n",
       "      <td>16.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>203965</td>\n",
       "      <td>289784.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fully_Paid</td>\n",
       "      <td>99999999</td>\n",
       "      <td>Short_Term</td>\n",
       "      <td>740.0</td>\n",
       "      <td>776188.0</td>\n",
       "      <td>&lt;_1_year</td>\n",
       "      <td>Own_Home</td>\n",
       "      <td>Debt_Consolidation</td>\n",
       "      <td>11578.22</td>\n",
       "      <td>8.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>134083</td>\n",
       "      <td>220220.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Loan_Status  Current_Loan_Amount        Term  Credit_Score  Annual_Income  \\\n",
       "0  Fully_Paid             99999999  Short_Term         741.0      2231892.0   \n",
       "1  Fully_Paid               217646  Short_Term         730.0      1184194.0   \n",
       "2  Fully_Paid               548746  Short_Term         678.0      2559110.0   \n",
       "3  Fully_Paid             99999999  Short_Term         728.0       714628.0   \n",
       "4  Fully_Paid             99999999  Short_Term         740.0       776188.0   \n",
       "\n",
       "  Years_in_current_job Home_Ownership             Purpose  Monthly_Debt  \\\n",
       "0              8_years       Own_Home  Debt_Consolidation      29200.53   \n",
       "1             <_1_year  Home_Mortgage  Debt_Consolidation      10855.08   \n",
       "2              2_years           Rent  Debt_Consolidation      18660.28   \n",
       "3              3_years           Rent  Debt_Consolidation      11851.06   \n",
       "4             <_1_year       Own_Home  Debt_Consolidation      11578.22   \n",
       "\n",
       "   Years_of_Credit_History  Months_since_last_delinquent  \\\n",
       "0                     14.9                          29.0   \n",
       "1                     19.6                          10.0   \n",
       "2                     22.6                          33.0   \n",
       "3                     16.0                          76.0   \n",
       "4                      8.5                          25.0   \n",
       "\n",
       "   Number_of_Open_Accounts  Number_of_Credit_Problems  Current_Credit_Balance  \\\n",
       "0                       18                          1                  297996   \n",
       "1                       13                          1                  122170   \n",
       "2                        4                          0                  437171   \n",
       "3                       16                          0                  203965   \n",
       "4                        6                          0                  134083   \n",
       "\n",
       "   Maximum_Open_Credit  Bankruptcies  Tax_Liens  \n",
       "0             750090.0           0.0        0.0  \n",
       "1             272052.0           1.0        0.0  \n",
       "2             555038.0           0.0        0.0  \n",
       "3             289784.0           0.0        0.0  \n",
       "4             220220.0           0.0        0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import our input dataset\n",
    "loans_df = pd.read_csv('Resources/loan_status.csv')\n",
    "loans_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22dc9bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_Status              2\n",
       "Term                     2\n",
       "Years_in_current_job    11\n",
       "Home_Ownership           4\n",
       "Purpose                  7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because both Scikit-Learn's RandomForestClassifier class and TensorFlow's Sequential class require preprocessing, we can perform our preprocessing steps on all of the data—no need to keep track of separate scaled and unscaled data. For our first preprocessing workflow, let's encode our categorical variables using Scikit-Learn's OneHotEncoder class.\n",
    "\n",
    "# First, we must make sure that none of our categorical variables require bucketing. To check this, let's get the column names of categorical variables and check their number of unique values. Add and run the following code to the notebook:\n",
    "\n",
    "# Generate our categorical variable list\n",
    "loans_cat = loans_df.dtypes[loans_df.dtypes == \"object\"].index.tolist()\n",
    "\n",
    "# Check the number of unique values in each column\n",
    "loans_df[loans_cat].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e8fae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10+_years    13149\n",
       "2_years       3225\n",
       "3_years       2997\n",
       "<_1_year      2699\n",
       "5_years       2487\n",
       "4_years       2286\n",
       "1_year        2247\n",
       "6_years       2109\n",
       "7_years       2082\n",
       "8_years       1675\n",
       "9_years       1467\n",
       "Name: Years_in_current_job, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at the number of unique values in our categorical variable, the \"Years_in_current_job\" column does have 11 unique values. Therefore, we should check the number of data points for each unique value to find out if any categorical variables can be bucketed together. Again, add and run the following code to the notebook:\n",
    "\n",
    "# Check the unique value counts to see if binning is required\n",
    "loans_df.Years_in_current_job.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020971f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taimurahmadkhan/opt/anaconda3/envs/mlenv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_Status_Fully_Paid</th>\n",
       "      <th>Loan_Status_Not_Paid</th>\n",
       "      <th>Term_Long_Term</th>\n",
       "      <th>Term_Short_Term</th>\n",
       "      <th>Years_in_current_job_10+_years</th>\n",
       "      <th>Years_in_current_job_1_year</th>\n",
       "      <th>Years_in_current_job_2_years</th>\n",
       "      <th>Years_in_current_job_3_years</th>\n",
       "      <th>Years_in_current_job_4_years</th>\n",
       "      <th>Years_in_current_job_5_years</th>\n",
       "      <th>...</th>\n",
       "      <th>Home_Ownership_Home_Mortgage</th>\n",
       "      <th>Home_Ownership_Own_Home</th>\n",
       "      <th>Home_Ownership_Rent</th>\n",
       "      <th>Purpose_Business_Loan</th>\n",
       "      <th>Purpose_Buy_House</th>\n",
       "      <th>Purpose_Buy_a_Car</th>\n",
       "      <th>Purpose_Debt_Consolidation</th>\n",
       "      <th>Purpose_Home_Improvements</th>\n",
       "      <th>Purpose_Medical_Bills</th>\n",
       "      <th>Purpose_Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Loan_Status_Fully_Paid  Loan_Status_Not_Paid  Term_Long_Term  \\\n",
       "0                     1.0                   0.0             0.0   \n",
       "1                     1.0                   0.0             0.0   \n",
       "2                     1.0                   0.0             0.0   \n",
       "3                     1.0                   0.0             0.0   \n",
       "4                     1.0                   0.0             0.0   \n",
       "\n",
       "   Term_Short_Term  Years_in_current_job_10+_years  \\\n",
       "0              1.0                             0.0   \n",
       "1              1.0                             0.0   \n",
       "2              1.0                             0.0   \n",
       "3              1.0                             0.0   \n",
       "4              1.0                             0.0   \n",
       "\n",
       "   Years_in_current_job_1_year  Years_in_current_job_2_years  \\\n",
       "0                          0.0                           0.0   \n",
       "1                          0.0                           0.0   \n",
       "2                          0.0                           1.0   \n",
       "3                          0.0                           0.0   \n",
       "4                          0.0                           0.0   \n",
       "\n",
       "   Years_in_current_job_3_years  Years_in_current_job_4_years  \\\n",
       "0                           0.0                           0.0   \n",
       "1                           0.0                           0.0   \n",
       "2                           0.0                           0.0   \n",
       "3                           1.0                           0.0   \n",
       "4                           0.0                           0.0   \n",
       "\n",
       "   Years_in_current_job_5_years  ...  Home_Ownership_Home_Mortgage  \\\n",
       "0                           0.0  ...                           0.0   \n",
       "1                           0.0  ...                           1.0   \n",
       "2                           0.0  ...                           0.0   \n",
       "3                           0.0  ...                           0.0   \n",
       "4                           0.0  ...                           0.0   \n",
       "\n",
       "   Home_Ownership_Own_Home  Home_Ownership_Rent  Purpose_Business_Loan  \\\n",
       "0                      1.0                  0.0                    0.0   \n",
       "1                      0.0                  0.0                    0.0   \n",
       "2                      0.0                  1.0                    0.0   \n",
       "3                      0.0                  1.0                    0.0   \n",
       "4                      1.0                  0.0                    0.0   \n",
       "\n",
       "   Purpose_Buy_House  Purpose_Buy_a_Car  Purpose_Debt_Consolidation  \\\n",
       "0                0.0                0.0                         1.0   \n",
       "1                0.0                0.0                         1.0   \n",
       "2                0.0                0.0                         1.0   \n",
       "3                0.0                0.0                         1.0   \n",
       "4                0.0                0.0                         1.0   \n",
       "\n",
       "   Purpose_Home_Improvements  Purpose_Medical_Bills  Purpose_Other  \n",
       "0                        0.0                    0.0            0.0  \n",
       "1                        0.0                    0.0            0.0  \n",
       "2                        0.0                    0.0            0.0  \n",
       "3                        0.0                    0.0            0.0  \n",
       "4                        0.0                    0.0            0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the number of data points for each unique value, all of the categorical values have a substantial number of data points. In this case, we have reason to leave the \"Years_in_current_job\" column alone because we don't want to bucket common values together and cause confusion in the model.\n",
    "\n",
    "# Since all of the categorical variables are ready for encoding, we can add and run the following code to the notebook:\n",
    "\n",
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(loans_df[loans_cat]))\n",
    "\n",
    "# Add the encoded variable names to the DataFrame\n",
    "encode_df.columns = enc.get_feature_names(loans_cat)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f517d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taimurahmadkhan/opt/anaconda3/envs/mlenv/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Current_Loan_Amount</th>\n",
       "      <th>Credit_Score</th>\n",
       "      <th>Annual_Income</th>\n",
       "      <th>Monthly_Debt</th>\n",
       "      <th>Years_of_Credit_History</th>\n",
       "      <th>Months_since_last_delinquent</th>\n",
       "      <th>Number_of_Open_Accounts</th>\n",
       "      <th>Number_of_Credit_Problems</th>\n",
       "      <th>Current_Credit_Balance</th>\n",
       "      <th>Maximum_Open_Credit</th>\n",
       "      <th>...</th>\n",
       "      <th>Home_Ownership_Home_Mortgage</th>\n",
       "      <th>Home_Ownership_Own_Home</th>\n",
       "      <th>Home_Ownership_Rent</th>\n",
       "      <th>Purpose_Business_Loan</th>\n",
       "      <th>Purpose_Buy_House</th>\n",
       "      <th>Purpose_Buy_a_Car</th>\n",
       "      <th>Purpose_Debt_Consolidation</th>\n",
       "      <th>Purpose_Home_Improvements</th>\n",
       "      <th>Purpose_Medical_Bills</th>\n",
       "      <th>Purpose_Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99999999</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2231892.0</td>\n",
       "      <td>29200.53</td>\n",
       "      <td>14.9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>297996</td>\n",
       "      <td>750090.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217646</td>\n",
       "      <td>730.0</td>\n",
       "      <td>1184194.0</td>\n",
       "      <td>10855.08</td>\n",
       "      <td>19.6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>122170</td>\n",
       "      <td>272052.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>548746</td>\n",
       "      <td>678.0</td>\n",
       "      <td>2559110.0</td>\n",
       "      <td>18660.28</td>\n",
       "      <td>22.6</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>437171</td>\n",
       "      <td>555038.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99999999</td>\n",
       "      <td>728.0</td>\n",
       "      <td>714628.0</td>\n",
       "      <td>11851.06</td>\n",
       "      <td>16.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>203965</td>\n",
       "      <td>289784.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99999999</td>\n",
       "      <td>740.0</td>\n",
       "      <td>776188.0</td>\n",
       "      <td>11578.22</td>\n",
       "      <td>8.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>134083</td>\n",
       "      <td>220220.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Current_Loan_Amount  Credit_Score  Annual_Income  Monthly_Debt  \\\n",
       "0             99999999         741.0      2231892.0      29200.53   \n",
       "1               217646         730.0      1184194.0      10855.08   \n",
       "2               548746         678.0      2559110.0      18660.28   \n",
       "3             99999999         728.0       714628.0      11851.06   \n",
       "4             99999999         740.0       776188.0      11578.22   \n",
       "\n",
       "   Years_of_Credit_History  Months_since_last_delinquent  \\\n",
       "0                     14.9                          29.0   \n",
       "1                     19.6                          10.0   \n",
       "2                     22.6                          33.0   \n",
       "3                     16.0                          76.0   \n",
       "4                      8.5                          25.0   \n",
       "\n",
       "   Number_of_Open_Accounts  Number_of_Credit_Problems  Current_Credit_Balance  \\\n",
       "0                       18                          1                  297996   \n",
       "1                       13                          1                  122170   \n",
       "2                        4                          0                  437171   \n",
       "3                       16                          0                  203965   \n",
       "4                        6                          0                  134083   \n",
       "\n",
       "   Maximum_Open_Credit  ...  Home_Ownership_Home_Mortgage  \\\n",
       "0             750090.0  ...                           0.0   \n",
       "1             272052.0  ...                           1.0   \n",
       "2             555038.0  ...                           0.0   \n",
       "3             289784.0  ...                           0.0   \n",
       "4             220220.0  ...                           0.0   \n",
       "\n",
       "   Home_Ownership_Own_Home  Home_Ownership_Rent  Purpose_Business_Loan  \\\n",
       "0                      1.0                  0.0                    0.0   \n",
       "1                      0.0                  0.0                    0.0   \n",
       "2                      0.0                  1.0                    0.0   \n",
       "3                      0.0                  1.0                    0.0   \n",
       "4                      1.0                  0.0                    0.0   \n",
       "\n",
       "   Purpose_Buy_House  Purpose_Buy_a_Car  Purpose_Debt_Consolidation  \\\n",
       "0                0.0                0.0                         1.0   \n",
       "1                0.0                0.0                         1.0   \n",
       "2                0.0                0.0                         1.0   \n",
       "3                0.0                0.0                         1.0   \n",
       "4                0.0                0.0                         1.0   \n",
       "\n",
       "   Purpose_Home_Improvements  Purpose_Medical_Bills  Purpose_Other  \n",
       "0                        0.0                    0.0            0.0  \n",
       "1                        0.0                    0.0            0.0  \n",
       "2                        0.0                    0.0            0.0  \n",
       "3                        0.0                    0.0            0.0  \n",
       "4                        0.0                    0.0            0.0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that our categorical variables have been encoded, we need to merge them back into our original data frame and remove the unencoded columns. To do this, add and run the following code in the notebook:\n",
    "\n",
    "# Merge one-hot encoded features and drop the originals\n",
    "loans_df = loans_df.merge(encode_df,left_index=True, right_index=True)\n",
    "loans_df = loans_df.drop(loans_cat,1)\n",
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe6b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we need to standardize our numerical variables using Scikit-Learn's StandardScaler class. Again, we must split our data into the training and testing sets prior to standardization to not incorporate the testing values into the scale. To perform the training/test split and standardize our numerical variables, add and run the following code in the notebook:\n",
    "\n",
    "# Remove loan status target from features data\n",
    "y = loans_df.Loan_Status_Fully_Paid.values\n",
    "X = loans_df.drop(columns=[\"Loan_Status_Fully_Paid\",\"Loan_Status_Not_Paid\"]).values\n",
    "\n",
    "# Split training/test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "040bb884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After standardizing variables in both the training and testing data, our dataset is ready for both models. First, we'll train and evaluate our random forest classifier.\n",
    "\n",
    "# REWIND\n",
    "# Random forest models can be built using Scikit-learn's RandomForestClassifierclass in the ensemblemodule.\n",
    "\n",
    "# For our purposes, we'll use a random forest classifier with the n_estimators parameter set to 128. Typically, 128 estimators is the largest number of estimators we would want to use in a model. To create our random forest classifier model and test the performance, add and run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83858045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Random forest predictive accuracy: 0.849\n"
     ]
    }
   ],
   "source": [
    "# Create a random forest classifier.\n",
    "rf_model = RandomForestClassifier(n_estimators=128, random_state=78)\n",
    "\n",
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "print(f\" Random forest predictive accuracy: {accuracy_score(y_test,y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "239ad215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27317 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 04:48:48.771647: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-28 04:48:48.771937: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27317/27317 [==============================] - 1s 49us/sample - loss: 0.4251 - accuracy: 0.8356\n",
      "Epoch 2/50\n",
      "27317/27317 [==============================] - 1s 29us/sample - loss: 0.3877 - accuracy: 0.8489\n",
      "Epoch 3/50\n",
      "27317/27317 [==============================] - 1s 36us/sample - loss: 0.3829 - accuracy: 0.8490\n",
      "Epoch 4/50\n",
      "27317/27317 [==============================] - 1s 38us/sample - loss: 0.3806 - accuracy: 0.8492\n",
      "Epoch 5/50\n",
      "27317/27317 [==============================] - 1s 29us/sample - loss: 0.3790 - accuracy: 0.8492\n",
      "Epoch 6/50\n",
      "27317/27317 [==============================] - 1s 27us/sample - loss: 0.3775 - accuracy: 0.8494\n",
      "Epoch 7/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3770 - accuracy: 0.8494\n",
      "Epoch 8/50\n",
      "27317/27317 [==============================] - 1s 27us/sample - loss: 0.3758 - accuracy: 0.8493\n",
      "Epoch 9/50\n",
      "27317/27317 [==============================] - 1s 27us/sample - loss: 0.3751 - accuracy: 0.8497\n",
      "Epoch 10/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3749 - accuracy: 0.8494\n",
      "Epoch 11/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3743 - accuracy: 0.8496\n",
      "Epoch 12/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3738 - accuracy: 0.8495\n",
      "Epoch 13/50\n",
      "27317/27317 [==============================] - 1s 29us/sample - loss: 0.3732 - accuracy: 0.8501\n",
      "Epoch 14/50\n",
      "27317/27317 [==============================] - 1s 27us/sample - loss: 0.3725 - accuracy: 0.8498\n",
      "Epoch 15/50\n",
      "27317/27317 [==============================] - 1s 27us/sample - loss: 0.3721 - accuracy: 0.8496\n",
      "Epoch 16/50\n",
      "27317/27317 [==============================] - 1s 27us/sample - loss: 0.3720 - accuracy: 0.8504\n",
      "Epoch 17/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3713 - accuracy: 0.8498\n",
      "Epoch 18/50\n",
      "27317/27317 [==============================] - 1s 27us/sample - loss: 0.3707 - accuracy: 0.8501\n",
      "Epoch 19/50\n",
      "27317/27317 [==============================] - 1s 27us/sample - loss: 0.3703 - accuracy: 0.8504\n",
      "Epoch 20/50\n",
      "27317/27317 [==============================] - 1s 27us/sample - loss: 0.3704 - accuracy: 0.8501\n",
      "Epoch 21/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3693 - accuracy: 0.8505\n",
      "Epoch 22/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3697 - accuracy: 0.8500\n",
      "Epoch 23/50\n",
      "27317/27317 [==============================] - 1s 35us/sample - loss: 0.3693 - accuracy: 0.8510\n",
      "Epoch 24/50\n",
      "27317/27317 [==============================] - 1s 29us/sample - loss: 0.3686 - accuracy: 0.8500\n",
      "Epoch 25/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3682 - accuracy: 0.8507\n",
      "Epoch 26/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3676 - accuracy: 0.8506\n",
      "Epoch 27/50\n",
      "27317/27317 [==============================] - 1s 27us/sample - loss: 0.3673 - accuracy: 0.8508\n",
      "Epoch 28/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3673 - accuracy: 0.8508\n",
      "Epoch 29/50\n",
      "27317/27317 [==============================] - 1s 30us/sample - loss: 0.3673 - accuracy: 0.8509\n",
      "Epoch 30/50\n",
      "27317/27317 [==============================] - 1s 27us/sample - loss: 0.3668 - accuracy: 0.8504\n",
      "Epoch 31/50\n",
      "27317/27317 [==============================] - 1s 27us/sample - loss: 0.3666 - accuracy: 0.8512\n",
      "Epoch 32/50\n",
      "27317/27317 [==============================] - 1s 29us/sample - loss: 0.3661 - accuracy: 0.8514\n",
      "Epoch 33/50\n",
      "27317/27317 [==============================] - 1s 35us/sample - loss: 0.3658 - accuracy: 0.8513\n",
      "Epoch 34/50\n",
      "27317/27317 [==============================] - 1s 29us/sample - loss: 0.3657 - accuracy: 0.8512\n",
      "Epoch 35/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3651 - accuracy: 0.8517\n",
      "Epoch 36/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3650 - accuracy: 0.8510\n",
      "Epoch 37/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3649 - accuracy: 0.8512\n",
      "Epoch 38/50\n",
      "27317/27317 [==============================] - 1s 29us/sample - loss: 0.3642 - accuracy: 0.8514\n",
      "Epoch 39/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3642 - accuracy: 0.8520\n",
      "Epoch 40/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3638 - accuracy: 0.8517\n",
      "Epoch 41/50\n",
      "27317/27317 [==============================] - 1s 29us/sample - loss: 0.3636 - accuracy: 0.8515\n",
      "Epoch 42/50\n",
      "27317/27317 [==============================] - 1s 30us/sample - loss: 0.3638 - accuracy: 0.8515\n",
      "Epoch 43/50\n",
      "27317/27317 [==============================] - 1s 29us/sample - loss: 0.3630 - accuracy: 0.8526\n",
      "Epoch 44/50\n",
      "27317/27317 [==============================] - 1s 29us/sample - loss: 0.3629 - accuracy: 0.8515\n",
      "Epoch 45/50\n",
      "27317/27317 [==============================] - 1s 29us/sample - loss: 0.3626 - accuracy: 0.8519\n",
      "Epoch 46/50\n",
      "27317/27317 [==============================] - 1s 30us/sample - loss: 0.3623 - accuracy: 0.8519\n",
      "Epoch 47/50\n",
      "27317/27317 [==============================] - 1s 29us/sample - loss: 0.3624 - accuracy: 0.8515\n",
      "Epoch 48/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3620 - accuracy: 0.8523\n",
      "Epoch 49/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3619 - accuracy: 0.8524\n",
      "Epoch 50/50\n",
      "27317/27317 [==============================] - 1s 28us/sample - loss: 0.3613 - accuracy: 0.8521\n",
      "9106/1 - 0s - loss: 0.4409 - accuracy: 0.8463\n",
      "Loss: 0.39249841517048983, Accuracy: 0.8462552428245544\n"
     ]
    }
   ],
   "source": [
    "# Next, we need to build, compile, and evaluate our deep learning model. Again, we'll use our typical binary classifier parameters:\n",
    "\n",
    "# Our first hidden layer will have an input_dim equal to 38, 24 neuron units, and will use the relu activation function.\n",
    "# Our second hidden layer will have 12 neuron unitsand also will use the relu activation function.\n",
    "# The loss function should be binary_crossentropy, using the adam optimizer.\n",
    "# Our model should provide the additional accuracy scoring metric and train over a maximum of 50 epochs.\n",
    "\n",
    "# To build and evaluate our deep learning model, we must add and run the following code to the notebook:\n",
    "\n",
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 24\n",
    "hidden_nodes_layer2 = 12\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=50)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b4ff38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, if we compare both model's predictive accuracy, their output is very similar. Both the random forest and deep learning models were able to predict correctly whether or not a loan will be repaid over 80% of the time. Although their predictive performance was comparable, their implementation and training times were not—the random forest classifier was able to train on the large dataset and predict values in seconds, while the deep learning model required a couple minutes to train on the tens of thousands of data points. In other words, the random forest model is able to achieve comparable predictive accuracy on large tabular data with less code and faster performance. The ultimate decision of whether to use a random forest versus a neural network comes down to preference. However, if your dataset is tabular, random forest is a great place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94591790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/geekculture/machine-learning-one-hot-encoding-vs-integer-encoding-f180eb831cf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6417640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
