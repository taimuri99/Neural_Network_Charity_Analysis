{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fce8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural networks, especially complex neural networks, are resource-hungry algorithms. When it comes to training neural networks on medium to large datasets, the amount of computation time to adequately train a model can take hours (or even days!) With simple modelling problems, like the ones covered in this module, training a model in the same notebook as an analysis is no problem. However, with more formal applications of neural network and deep learning models, data scientists cannot afford the time or resources to build and train a model each time they analyze data. In these cases, a trained model must be stored and accessed outside of the training environment.\n",
    "\n",
    "# With TensorFlow, we have the ability to save and load neural network models at any stage, including partially trained models. When building a TensorFlow model, if we use Keras' ModelCheckpoint method, we can save the model weights after it tests a set number of data points. Then, at any point, we can reload the checkpoint weights and resume model training. Saving checkpoints while training has a number of benefits:\n",
    "\n",
    "# We can short-circuit our training loop at any time (stop the function by pressing CTRL+C, or by pressing the stop button at the top of the notebook). This can be helpful if the model is showing signs of overfitting.\n",
    "# The model is protected from computer problems (power failure, computer crash, etc.). Worst-case scenario: We would lose five epochs' worth of optimization.\n",
    "# We can restore previous model weight coefficients to try and revert overfitting.\n",
    "# Let's practice generating checkpoint files and loading model weights from different epochs. To make things simple, we'll implement checkpoints to our previous deep learning example notebook. To start, we'll open our \"DeepLearning_Tabular\" (or whatever similar name you may have used) notebook and rerun all of the code in the notebook for the following steps:\n",
    "\n",
    "# Import dependencies.\n",
    "# Import the input dataset.\n",
    "# Generate categorical variable list.\n",
    "# Create a OneHotEncoder instance.\n",
    "# Fit and transform the OneHotEncoder.\n",
    "# Add the encoded variable names to the DataFrame.\n",
    "# Merge one-hot encoded features and drop the originals.\n",
    "# Split the preprocessed data into features and target arrays.\n",
    "# Split the preprocessed data into training and testing dataset.\n",
    "# Create a StandardScaler instance.\n",
    "# Fit the StandardScaler.\n",
    "# Scale the data.\n",
    "# Define the model.\n",
    "# Add first and second hidden layers.\n",
    "# Add the output layer.\n",
    "# Check the structure of the model.\n",
    "# REWIND\n",
    "# You have coded all of these steps within this module. If you get stuck, or something is not working, try going back to earlier sections and recopy the code blocks.\n",
    "\n",
    "# Now that we have our training data ready, we can implement checkpoints to our deep learning model.\n",
    "\n",
    "\n",
    "\n",
    "# Now that we have our training data and our model defined, we're ready to compile and train our model using checkpoints. To use checkpoints, we need to define the checkpoint file name and directory path. For our purposes, we'll label our checkpoints by epoch number and contain them within their own folder. This ensures that our checkpoint files are neat, organized, and easily identifiable. Add and run the following code to our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a86c2009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taimurahmadkhan/opt/anaconda3/envs/mlenv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Users/taimurahmadkhan/opt/anaconda3/envs/mlenv/lib/python3.7/site-packages/ipykernel_launcher.py:25: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "/Users/taimurahmadkhan/opt/anaconda3/envs/mlenv/lib/python3.7/site-packages/ipykernel_launcher.py:29: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"
     ]
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import our input dataset\n",
    "attrition_df = pd.read_csv('Resources/HR-Employee-Attrition.csv')\n",
    "#v Looking at the top of our DataFrame, there are multiple columns with categorical values as well as our numerical values. To make things easier, we should generate a list of categorical variable names. Instead of searching across all 35 columns and keeping track of which variables need categorical preprocessing, we'll let Python do all of the heavy lifting. As an added bonus, we can use our variable list to perform the one-hot encoding once, rather than for each individual variable. To generate our variable list, we'll use Pandas Dataframe.dtypes property. Add and run the following code in your notebook:\n",
    "\n",
    "# Generate our categorical variable list\n",
    "attrition_cat = attrition_df.dtypes[attrition_df.dtypes == \"object\"].index.tolist()\n",
    "\n",
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(attrition_df[attrition_cat]))\n",
    "\n",
    "# Add the encoded variable names to the DataFrame\n",
    "encode_df.columns = enc.get_feature_names(attrition_cat)\n",
    "\n",
    "# Merge one-hot encoded features and drop the originals\n",
    "attrition_df = attrition_df.merge(encode_df,left_index=True, right_index=True)\n",
    "attrition_df = attrition_df.drop(attrition_cat,1)\n",
    "\n",
    "# Split our preprocessed data into our features and target arrays\n",
    "y = attrition_df[\"Attrition_Yes\"].values\n",
    "X = attrition_df.drop([\"Attrition_Yes\",\"Attrition_No\"],1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8284d9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 448       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 45        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 499\n",
      "Trainable params: 499\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 05:01:15.546750: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-28 05:01:15.549767: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 8\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed4d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import checkpoint dependencies\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"checkpoints/\",exist_ok=True)\n",
    "checkpoint_path = \"checkpoints/weights.{epoch:02d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5a7a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we have defined the file structure and filepath, we need to create a callback object for our deep learning model. A callback object is used in the Keras module to define a set of functions that will be applied at specific stages of the training process. There are a number of different callback functions available that can create log files, force training to stop, send training status messages, or in our case save model checkpoints. To create an effective checkpoint callback using the ModelCheckpoint method, we need to provide the following parameters:\n",
    "\n",
    "# filepath=checkpoint_path—the checkpoint directory and file structure we defined previously\n",
    "# verbose=1—we'll be notified when a checkpoint is being saved to the directory\n",
    "# save_weights_only=True—saving the full model each time can fill up a hard drive very quickly; this ensures that the checkpoint files take up minimal space\n",
    "# save_freq='epoch'—checkpoints will be saved every epoch\n",
    "# Bringing it all together, we can compile, train, and evaluate our deep learning model by adding and running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8413f8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1102 samples\n",
      "Epoch 1/100\n",
      "  32/1102 [..............................] - ETA: 19s - loss: 1.0987 - accuracy: 0.2500\n",
      "Epoch 00001: saving model to checkpoints/weights.01.hdf5\n",
      "1102/1102 [==============================] - 1s 572us/sample - loss: 0.9551 - accuracy: 0.3330\n",
      "Epoch 2/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.7420 - accuracy: 0.4375\n",
      "Epoch 00002: saving model to checkpoints/weights.02.hdf5\n",
      "1102/1102 [==============================] - 0s 37us/sample - loss: 0.7592 - accuracy: 0.4782\n",
      "Epoch 3/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.7032 - accuracy: 0.4062\n",
      "Epoch 00003: saving model to checkpoints/weights.03.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.6491 - accuracy: 0.6343\n",
      "Epoch 4/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.6164 - accuracy: 0.7500\n",
      "Epoch 00004: saving model to checkpoints/weights.04.hdf5\n",
      "1102/1102 [==============================] - 0s 36us/sample - loss: 0.5823 - accuracy: 0.7495\n",
      "Epoch 5/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.5025 - accuracy: 0.7812\n",
      "Epoch 00005: saving model to checkpoints/weights.05.hdf5\n",
      "1102/1102 [==============================] - 0s 35us/sample - loss: 0.5363 - accuracy: 0.7895\n",
      "Epoch 6/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.5079 - accuracy: 0.8125\n",
      "Epoch 00006: saving model to checkpoints/weights.06.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.5041 - accuracy: 0.8085\n",
      "Epoch 7/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.4942 - accuracy: 0.8750\n",
      "Epoch 00007: saving model to checkpoints/weights.07.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.4776 - accuracy: 0.8240\n",
      "Epoch 8/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.4143 - accuracy: 0.8125\n",
      "Epoch 00008: saving model to checkpoints/weights.08.hdf5\n",
      "1102/1102 [==============================] - 0s 46us/sample - loss: 0.4566 - accuracy: 0.8294\n",
      "Epoch 9/100\n",
      " 960/1102 [=========================>....] - ETA: 0s - loss: 0.4454 - accuracy: 0.8292\n",
      "Epoch 00009: saving model to checkpoints/weights.09.hdf5\n",
      "1102/1102 [==============================] - 0s 59us/sample - loss: 0.4381 - accuracy: 0.8348\n",
      "Epoch 10/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3767 - accuracy: 0.8750\n",
      "Epoch 00010: saving model to checkpoints/weights.10.hdf5\n",
      "1102/1102 [==============================] - 0s 36us/sample - loss: 0.4218 - accuracy: 0.8376\n",
      "Epoch 11/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.4146 - accuracy: 0.8438\n",
      "Epoch 00011: saving model to checkpoints/weights.11.hdf5\n",
      "1102/1102 [==============================] - 0s 35us/sample - loss: 0.4076 - accuracy: 0.8421\n",
      "Epoch 12/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3906 - accuracy: 0.9375\n",
      "Epoch 00012: saving model to checkpoints/weights.12.hdf5\n",
      "1102/1102 [==============================] - 0s 36us/sample - loss: 0.3938 - accuracy: 0.8466\n",
      "Epoch 13/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3910 - accuracy: 0.7812\n",
      "Epoch 00013: saving model to checkpoints/weights.13.hdf5\n",
      "1102/1102 [==============================] - 0s 45us/sample - loss: 0.3816 - accuracy: 0.8512\n",
      "Epoch 14/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2624 - accuracy: 0.9688\n",
      "Epoch 00014: saving model to checkpoints/weights.14.hdf5\n",
      "1102/1102 [==============================] - 0s 36us/sample - loss: 0.3703 - accuracy: 0.8548\n",
      "Epoch 15/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3495 - accuracy: 0.8750\n",
      "Epoch 00015: saving model to checkpoints/weights.15.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.3598 - accuracy: 0.8612\n",
      "Epoch 16/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3140 - accuracy: 0.8438\n",
      "Epoch 00016: saving model to checkpoints/weights.16.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.3506 - accuracy: 0.8648\n",
      "Epoch 17/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3951 - accuracy: 0.8125\n",
      "Epoch 00017: saving model to checkpoints/weights.17.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.3420 - accuracy: 0.8684\n",
      "Epoch 18/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.5601 - accuracy: 0.8438\n",
      "Epoch 00018: saving model to checkpoints/weights.18.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.3342 - accuracy: 0.8711\n",
      "Epoch 19/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2160 - accuracy: 0.9375\n",
      "Epoch 00019: saving model to checkpoints/weights.19.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.3271 - accuracy: 0.8730\n",
      "Epoch 20/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.4317 - accuracy: 0.8125\n",
      "Epoch 00020: saving model to checkpoints/weights.20.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.3201 - accuracy: 0.8793\n",
      "Epoch 21/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2898 - accuracy: 0.9375\n",
      "Epoch 00021: saving model to checkpoints/weights.21.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.3134 - accuracy: 0.8857\n",
      "Epoch 22/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3658 - accuracy: 0.8125\n",
      "Epoch 00022: saving model to checkpoints/weights.22.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.3071 - accuracy: 0.8875\n",
      "Epoch 23/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3235 - accuracy: 0.9062\n",
      "Epoch 00023: saving model to checkpoints/weights.23.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.3008 - accuracy: 0.8902\n",
      "Epoch 24/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3369 - accuracy: 0.8750\n",
      "Epoch 00024: saving model to checkpoints/weights.24.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.2956 - accuracy: 0.8966\n",
      "Epoch 25/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3698 - accuracy: 0.8750\n",
      "Epoch 00025: saving model to checkpoints/weights.25.hdf5\n",
      "1102/1102 [==============================] - 0s 41us/sample - loss: 0.2900 - accuracy: 0.8984\n",
      "Epoch 26/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2486 - accuracy: 0.9375\n",
      "Epoch 00026: saving model to checkpoints/weights.26.hdf5\n",
      "1102/1102 [==============================] - 0s 36us/sample - loss: 0.2852 - accuracy: 0.9011\n",
      "Epoch 27/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2508 - accuracy: 0.9062\n",
      "Epoch 00027: saving model to checkpoints/weights.27.hdf5\n",
      "1102/1102 [==============================] - 0s 35us/sample - loss: 0.2804 - accuracy: 0.9038\n",
      "Epoch 28/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3391 - accuracy: 0.8438\n",
      "Epoch 00028: saving model to checkpoints/weights.28.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.2770 - accuracy: 0.9056\n",
      "Epoch 29/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2937 - accuracy: 0.8438\n",
      "Epoch 00029: saving model to checkpoints/weights.29.hdf5\n",
      "1102/1102 [==============================] - 0s 43us/sample - loss: 0.2715 - accuracy: 0.9056\n",
      "Epoch 30/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3107 - accuracy: 0.8438\n",
      "Epoch 00030: saving model to checkpoints/weights.30.hdf5\n",
      "1102/1102 [==============================] - 0s 43us/sample - loss: 0.2684 - accuracy: 0.9093\n",
      "Epoch 31/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2866 - accuracy: 0.8750\n",
      "Epoch 00031: saving model to checkpoints/weights.31.hdf5\n",
      "1102/1102 [==============================] - 0s 35us/sample - loss: 0.2644 - accuracy: 0.9102\n",
      "Epoch 32/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3094 - accuracy: 0.8750\n",
      "Epoch 00032: saving model to checkpoints/weights.32.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.2607 - accuracy: 0.9102\n",
      "Epoch 33/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2699 - accuracy: 0.9375\n",
      "Epoch 00033: saving model to checkpoints/weights.33.hdf5\n",
      "1102/1102 [==============================] - 0s 35us/sample - loss: 0.2583 - accuracy: 0.9093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1557 - accuracy: 0.9688\n",
      "Epoch 00034: saving model to checkpoints/weights.34.hdf5\n",
      "1102/1102 [==============================] - 0s 36us/sample - loss: 0.2542 - accuracy: 0.9120\n",
      "Epoch 35/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1812 - accuracy: 0.9375\n",
      "Epoch 00035: saving model to checkpoints/weights.35.hdf5\n",
      "1102/1102 [==============================] - 0s 36us/sample - loss: 0.2514 - accuracy: 0.9111\n",
      "Epoch 36/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2468 - accuracy: 0.8750\n",
      "Epoch 00036: saving model to checkpoints/weights.36.hdf5\n",
      "1102/1102 [==============================] - 0s 36us/sample - loss: 0.2485 - accuracy: 0.9138\n",
      "Epoch 37/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1904 - accuracy: 0.9375\n",
      "Epoch 00037: saving model to checkpoints/weights.37.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.2456 - accuracy: 0.9147\n",
      "Epoch 38/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2584 - accuracy: 0.9375\n",
      "Epoch 00038: saving model to checkpoints/weights.38.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.2428 - accuracy: 0.9156\n",
      "Epoch 39/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2682 - accuracy: 0.9375\n",
      "Epoch 00039: saving model to checkpoints/weights.39.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.2396 - accuracy: 0.9156\n",
      "Epoch 40/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.6049 - accuracy: 0.7500\n",
      "Epoch 00040: saving model to checkpoints/weights.40.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.2373 - accuracy: 0.9183\n",
      "Epoch 41/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3995 - accuracy: 0.8438\n",
      "Epoch 00041: saving model to checkpoints/weights.41.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.2347 - accuracy: 0.9174\n",
      "Epoch 42/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3366 - accuracy: 0.8750\n",
      "Epoch 00042: saving model to checkpoints/weights.42.hdf5\n",
      "1102/1102 [==============================] - 0s 35us/sample - loss: 0.2323 - accuracy: 0.9174\n",
      "Epoch 43/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2271 - accuracy: 0.8750\n",
      "Epoch 00043: saving model to checkpoints/weights.43.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.2299 - accuracy: 0.9211\n",
      "Epoch 44/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2356 - accuracy: 0.9375\n",
      "Epoch 00044: saving model to checkpoints/weights.44.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.2277 - accuracy: 0.9192\n",
      "Epoch 45/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2391 - accuracy: 0.9062\n",
      "Epoch 00045: saving model to checkpoints/weights.45.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.2252 - accuracy: 0.9238\n",
      "Epoch 46/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1198 - accuracy: 0.9688\n",
      "Epoch 00046: saving model to checkpoints/weights.46.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.2236 - accuracy: 0.9238\n",
      "Epoch 47/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2170 - accuracy: 0.8750\n",
      "Epoch 00047: saving model to checkpoints/weights.47.hdf5\n",
      "1102/1102 [==============================] - 0s 38us/sample - loss: 0.2208 - accuracy: 0.9238\n",
      "Epoch 48/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1880 - accuracy: 0.9375\n",
      "Epoch 00048: saving model to checkpoints/weights.48.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.2187 - accuracy: 0.9247\n",
      "Epoch 49/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.4482 - accuracy: 0.8125\n",
      "Epoch 00049: saving model to checkpoints/weights.49.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.2170 - accuracy: 0.9256\n",
      "Epoch 50/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3385 - accuracy: 0.8750\n",
      "Epoch 00050: saving model to checkpoints/weights.50.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.2155 - accuracy: 0.9265\n",
      "Epoch 51/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1067 - accuracy: 1.0000\n",
      "Epoch 00051: saving model to checkpoints/weights.51.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.2136 - accuracy: 0.9292\n",
      "Epoch 52/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2897 - accuracy: 0.9062\n",
      "Epoch 00052: saving model to checkpoints/weights.52.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.2116 - accuracy: 0.9256\n",
      "Epoch 53/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3119 - accuracy: 0.9062\n",
      "Epoch 00053: saving model to checkpoints/weights.53.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.2097 - accuracy: 0.9274\n",
      "Epoch 54/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1790 - accuracy: 0.9688\n",
      "Epoch 00054: saving model to checkpoints/weights.54.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.2078 - accuracy: 0.9301\n",
      "Epoch 55/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1739 - accuracy: 0.9375\n",
      "Epoch 00055: saving model to checkpoints/weights.55.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.2073 - accuracy: 0.9310\n",
      "Epoch 56/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2110 - accuracy: 0.9688\n",
      "Epoch 00056: saving model to checkpoints/weights.56.hdf5\n",
      "1102/1102 [==============================] - 0s 37us/sample - loss: 0.2047 - accuracy: 0.9338\n",
      "Epoch 57/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1841 - accuracy: 0.9062\n",
      "Epoch 00057: saving model to checkpoints/weights.57.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.2041 - accuracy: 0.9338\n",
      "Epoch 58/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2704 - accuracy: 0.9062\n",
      "Epoch 00058: saving model to checkpoints/weights.58.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.2018 - accuracy: 0.9301\n",
      "Epoch 59/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1823 - accuracy: 0.9688\n",
      "Epoch 00059: saving model to checkpoints/weights.59.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.2001 - accuracy: 0.9365\n",
      "Epoch 60/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1239 - accuracy: 1.0000\n",
      "Epoch 00060: saving model to checkpoints/weights.60.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.1987 - accuracy: 0.9347\n",
      "Epoch 61/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1220 - accuracy: 0.9688\n",
      "Epoch 00061: saving model to checkpoints/weights.61.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.1981 - accuracy: 0.9338\n",
      "Epoch 62/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.3129 - accuracy: 0.8750\n",
      "Epoch 00062: saving model to checkpoints/weights.62.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.1964 - accuracy: 0.9338\n",
      "Epoch 63/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2780 - accuracy: 0.9062\n",
      "Epoch 00063: saving model to checkpoints/weights.63.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.1945 - accuracy: 0.9365\n",
      "Epoch 64/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1930 - accuracy: 0.9375\n",
      "Epoch 00064: saving model to checkpoints/weights.64.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.1928 - accuracy: 0.9374\n",
      "Epoch 65/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1969 - accuracy: 0.9375\n",
      "Epoch 00065: saving model to checkpoints/weights.65.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.1915 - accuracy: 0.9347\n",
      "Epoch 66/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2055 - accuracy: 0.9375\n",
      "Epoch 00066: saving model to checkpoints/weights.66.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.1903 - accuracy: 0.9383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2554 - accuracy: 0.9062\n",
      "Epoch 00067: saving model to checkpoints/weights.67.hdf5\n",
      "1102/1102 [==============================] - 0s 42us/sample - loss: 0.1890 - accuracy: 0.9392\n",
      "Epoch 68/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1595 - accuracy: 0.9688\n",
      "Epoch 00068: saving model to checkpoints/weights.68.hdf5\n",
      "1102/1102 [==============================] - 0s 36us/sample - loss: 0.1871 - accuracy: 0.9392\n",
      "Epoch 69/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2942 - accuracy: 0.9062\n",
      "Epoch 00069: saving model to checkpoints/weights.69.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.1861 - accuracy: 0.9401\n",
      "Epoch 70/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1686 - accuracy: 0.9062\n",
      "Epoch 00070: saving model to checkpoints/weights.70.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.1841 - accuracy: 0.9410\n",
      "Epoch 71/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1105 - accuracy: 0.9688\n",
      "Epoch 00071: saving model to checkpoints/weights.71.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.1815 - accuracy: 0.9392\n",
      "Epoch 72/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.0617 - accuracy: 1.0000\n",
      "Epoch 00072: saving model to checkpoints/weights.72.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.1802 - accuracy: 0.9428\n",
      "Epoch 73/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1917 - accuracy: 0.9062\n",
      "Epoch 00073: saving model to checkpoints/weights.73.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.1787 - accuracy: 0.9419\n",
      "Epoch 74/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1137 - accuracy: 1.0000\n",
      "Epoch 00074: saving model to checkpoints/weights.74.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.1775 - accuracy: 0.9374\n",
      "Epoch 75/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1489 - accuracy: 0.9688\n",
      "Epoch 00075: saving model to checkpoints/weights.75.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.1762 - accuracy: 0.9437\n",
      "Epoch 76/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.0764 - accuracy: 1.0000\n",
      "Epoch 00076: saving model to checkpoints/weights.76.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.1749 - accuracy: 0.9419\n",
      "Epoch 77/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2764 - accuracy: 0.9062\n",
      "Epoch 00077: saving model to checkpoints/weights.77.hdf5\n",
      "1102/1102 [==============================] - 0s 35us/sample - loss: 0.1729 - accuracy: 0.9446\n",
      "Epoch 78/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1940 - accuracy: 0.9062\n",
      "Epoch 00078: saving model to checkpoints/weights.78.hdf5\n",
      "1102/1102 [==============================] - 0s 35us/sample - loss: 0.1723 - accuracy: 0.9446\n",
      "Epoch 79/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1114 - accuracy: 0.9688\n",
      "Epoch 00079: saving model to checkpoints/weights.79.hdf5\n",
      "1102/1102 [==============================] - 0s 35us/sample - loss: 0.1702 - accuracy: 0.9456\n",
      "Epoch 80/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.0917 - accuracy: 1.0000\n",
      "Epoch 00080: saving model to checkpoints/weights.80.hdf5\n",
      "1102/1102 [==============================] - 0s 35us/sample - loss: 0.1689 - accuracy: 0.9456\n",
      "Epoch 81/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2017 - accuracy: 0.8750\n",
      "Epoch 00081: saving model to checkpoints/weights.81.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.1681 - accuracy: 0.9456\n",
      "Epoch 82/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2347 - accuracy: 0.9688\n",
      "Epoch 00082: saving model to checkpoints/weights.82.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.1661 - accuracy: 0.9446\n",
      "Epoch 83/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1694 - accuracy: 0.9375\n",
      "Epoch 00083: saving model to checkpoints/weights.83.hdf5\n",
      "1102/1102 [==============================] - 0s 35us/sample - loss: 0.1660 - accuracy: 0.9456\n",
      "Epoch 84/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1949 - accuracy: 0.9375\n",
      "Epoch 00084: saving model to checkpoints/weights.84.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.1634 - accuracy: 0.9419\n",
      "Epoch 85/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2174 - accuracy: 0.9062\n",
      "Epoch 00085: saving model to checkpoints/weights.85.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.1633 - accuracy: 0.9474\n",
      "Epoch 86/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1738 - accuracy: 0.9688\n",
      "Epoch 00086: saving model to checkpoints/weights.86.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.1618 - accuracy: 0.9483\n",
      "Epoch 87/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1366 - accuracy: 0.9375\n",
      "Epoch 00087: saving model to checkpoints/weights.87.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.1603 - accuracy: 0.9501\n",
      "Epoch 88/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2061 - accuracy: 0.9375\n",
      "Epoch 00088: saving model to checkpoints/weights.88.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.1599 - accuracy: 0.9474\n",
      "Epoch 89/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1538 - accuracy: 0.9375\n",
      "Epoch 00089: saving model to checkpoints/weights.89.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.1570 - accuracy: 0.9555\n",
      "Epoch 90/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1289 - accuracy: 0.9688\n",
      "Epoch 00090: saving model to checkpoints/weights.90.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.1563 - accuracy: 0.9528\n",
      "Epoch 91/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1057 - accuracy: 0.9688\n",
      "Epoch 00091: saving model to checkpoints/weights.91.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.1550 - accuracy: 0.9528\n",
      "Epoch 92/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.0934 - accuracy: 1.0000\n",
      "Epoch 00092: saving model to checkpoints/weights.92.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.1532 - accuracy: 0.9519\n",
      "Epoch 93/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.0995 - accuracy: 0.9688\n",
      "Epoch 00093: saving model to checkpoints/weights.93.hdf5\n",
      "1102/1102 [==============================] - 0s 32us/sample - loss: 0.1528 - accuracy: 0.9519\n",
      "Epoch 94/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1181 - accuracy: 0.9688\n",
      "Epoch 00094: saving model to checkpoints/weights.94.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.1510 - accuracy: 0.9510\n",
      "Epoch 95/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1118 - accuracy: 0.9688\n",
      "Epoch 00095: saving model to checkpoints/weights.95.hdf5\n",
      "1102/1102 [==============================] - 0s 35us/sample - loss: 0.1502 - accuracy: 0.9537\n",
      "Epoch 96/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.2517 - accuracy: 0.9062\n",
      "Epoch 00096: saving model to checkpoints/weights.96.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.1494 - accuracy: 0.9546\n",
      "Epoch 97/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.1421 - accuracy: 0.9375\n",
      "Epoch 00097: saving model to checkpoints/weights.97.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.1476 - accuracy: 0.9528\n",
      "Epoch 98/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.0585 - accuracy: 1.0000\n",
      "Epoch 00098: saving model to checkpoints/weights.98.hdf5\n",
      "1102/1102 [==============================] - 0s 33us/sample - loss: 0.1462 - accuracy: 0.9564\n",
      "Epoch 99/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.0753 - accuracy: 1.0000\n",
      "Epoch 00099: saving model to checkpoints/weights.99.hdf5\n",
      "1102/1102 [==============================] - 0s 35us/sample - loss: 0.1454 - accuracy: 0.9564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "  32/1102 [..............................] - ETA: 0s - loss: 0.0855 - accuracy: 1.0000\n",
      "Epoch 00100: saving model to checkpoints/weights.100.hdf5\n",
      "1102/1102 [==============================] - 0s 34us/sample - loss: 0.1454 - accuracy: 0.9583\n",
      "368/1 - 0s - loss: 0.2961 - accuracy: 0.8533\n",
      "Loss: 0.4365652022154435, Accuracy: 0.85326087474823\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Create a callback that saves the model's weights every epoch\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch')\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=100,callbacks=[cp_callback])\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ddac7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running the previous code, we have created our trained model within the Python session, as well as a folder of checkpoints we can use to restore previous model weights. Now if we ever need to restore weights, we can use the Keras Sequential model's load_weights method to restore the model weights. To test this functionality, let's define another deep learning model, but restore the weights using the checkpoints rather than training the model. Once again we must add and run the following to our notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc9ec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368/1 - 0s - loss: 0.2961 - accuracy: 0.8533\n",
      "Loss: 0.4365652022154435, Accuracy: 0.85326087474823\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 8\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "nn_new = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_new.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn_new.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_new.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "nn_new.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Restore the model weights\n",
    "nn_new.load_weights(\"checkpoints/weights.100.hdf5\")\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_new.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "488f6d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the checkpoints, we were able to regenerate the model instantaneously and confirm the model was able to produce the exact same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28e3bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having the weights is great, but what about saving—and sharing—the entire model? Is that possible? Beks is determined to find out.\n",
    "# Checkpoints are a great way to save model weights during training, but they fall short when it comes to sharing a trained model. In data science, trained models are published in scientific papers, deployed in software, open-sourced on GitHub, not to mention passed along to colleagues. In these cases, it is not practical to pass along only model weights, which can cause frustration and confusion. Instead, we can use the Keras Sequential model's save method to export the entire model (weights, structure, and configuration settings) to an Hierarchical Data Format (HDF5Links to an external site.) file. Once saved, anyone can import the exact same trained model to their environment by using the Keras load_model method and use it for analysis.\n",
    "\n",
    "# NOTE\n",
    "# Even though we can save full neural network and deep learning models using Keras checkpoints, each full model file is almost ten times the size of a weight-only file. For those with limited hard drive space, saving full models using checkpoints is not feasible.\n",
    "\n",
    "# To practice exporting and importing our entire model, we'll use the same notebook as our previous section.\n",
    "\n",
    "# Currently, in our notebook environment, we should have a fully trained classification model that can predict employee attrition based on features in the dataset. To export the trained model, we need to add and run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef4e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "nn_new.save(\"trained_attrition.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a0d8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running the code, we should see a file named \"trained_attrition.h5,\" which contains the complete model and configuration. Now that we have the model saved, we can create the model at any point. Let's try importing the model into the notebook without providing any structure or context. To import the model, add and run the following code:\n",
    "\n",
    "# Import the model to a new object\n",
    "nn_imported = tf.keras.models.load_model('trained_attrition.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a608279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368/1 - 0s - loss: 0.2961 - accuracy: 0.8533\n",
      "Loss: 0.4365652022154435, Accuracy: 0.85326087474823\n"
     ]
    }
   ],
   "source": [
    "# Lastly, we can test the performance of the completed model on our test dataset by adding and running the following code:\n",
    "\n",
    "# Evaluate the completed model using the test data\n",
    "model_loss, model_accuracy = nn_imported.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "662bd975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the final results, our imported model was able to reproduce the exact same performance metrics as the original model. Using this same procedure, we can import any type of Keras model for evaluation on a dataset with the same features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7f127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
